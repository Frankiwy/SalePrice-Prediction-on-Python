{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices: Advanced Regression Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import defaultdict \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn import preprocessing\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.  Data Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import data set and understand dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = df_train.dtypes[df_train.dtypes != \"object\"].index.tolist() # list with all numerical features names\n",
    "categorical_features = df_train.dtypes[df_train.dtypes == \"object\"].index.tolist()# list with all categorical features names\n",
    "\n",
    "\n",
    "print(\"DataFrame Train shape: \"+str(df_train.shape),\n",
    "      \"Number of Categorical features: \"+ str(len(categorical_features)), \n",
    "      \"Number of Numerical features: \"+str(len(numerical_features)),\n",
    "      sep='\\n')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>**Return some information about numerical and categorical feaures:**</font> \n",
    "- The first one returns information about numerical features\n",
    "- The second one returns information about categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns some information about numerical variables .\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns information about categorical features\n",
    "df_train.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Managing _\"NaN\"_ Values and delete _Outliers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return all the features with missing values\n",
    "def missing_values_function(df):\n",
    "    \n",
    "    # new df where there are all missing values ordered in an ascending order\n",
    "    missing = df.isnull().sum().sort_values(ascending=False)\n",
    "    #compute the % of missing values per each feature\n",
    "    missing_percent = round(((df.isnull().sum()/df.isnull().count())*100),2).sort_values(ascending=False)\n",
    "    #store into a new df that we we'll use to plot and show\n",
    "    missing_df = pd.concat([missing, missing_percent], axis=1, keys=['missing', '%'])\n",
    "    \n",
    "    axes_drop = list() # list of all the columns (the features names) with missing values\n",
    "    \n",
    "    # drop rows with no missing values (rows because on the index are reported the features)\n",
    "    for e,v in enumerate(missing_df['missing']): \n",
    "    \n",
    "        if v == 0:\n",
    "            axes_drop.append(missing_df.index.values[e])# get the index name and store into \"axes_drop\"\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "    missing_df.drop(axes_drop, inplace = True) # delete all the rows with no missing values\n",
    "    missing_df = missing_df.reset_index() # reset the index in order to have numbers instead of features names\n",
    "    missing_df = missing_df.rename(columns = {'index':'feature_name'}) # rename the 'index' column\n",
    "    \n",
    "    return (missing_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = missing_values_function (df_train) # DataFrane containing all features with missing values\n",
    "\n",
    "all_nan = missing_df['feature_name'].tolist() #list of features with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Histogram_NaN_Values(df, real, size, x_n):\n",
    "    \n",
    "    # figure shows percentage of NaN values per each feature \n",
    "\n",
    "    fig = plt.figure(figsize=(30,15))\n",
    "    if real== 'yes':\n",
    "        fig.suptitle('True Features with \\'Nan\\'', fontsize=30)\n",
    "\n",
    "    ax = sns.barplot(x = \"feature_name\", y = \"missing\", data = missing_df)\n",
    "    ax.set(ylabel = 'number of \\'Nan\\'')\n",
    "    ax.set_xticklabels(labels = missing_df[\"feature_name\"], fontsize=20, rotation=80)\n",
    "    #ax.xaxis.labelpad = 15\n",
    "    #ax.yaxis.labelpad = 15\n",
    "    ax.xaxis.label.set_size(25)\n",
    "    ax.yaxis.label.set_size(25)\n",
    "    ax.xaxis.label.set_color('red')\n",
    "    ax.yaxis.label.set_color('red')\n",
    "    ax.tick_params(axis='both', labelsize=20) \n",
    "\n",
    "\n",
    "    for e,p in enumerate(ax.patches):\n",
    "            percentage = str(missing_df['%'][e])+'%'\n",
    "            x = p.get_x() + x_n # get x coordinate\n",
    "            y = p.get_y() + p.get_height() + 2 # get y coordinate\n",
    "            ax.annotate(percentage, (x, y),fontsize=size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Histogram_NaN_Values(missing_df, real = 'no', size = 20, x_n = 0)\n",
    "#plt.savefig('Plots/Histogram all NaN.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__As we can see from the picture above, there are many features with a lot of missing values. However, not for all of them meaning that the data have been wrongly recorded but means that the houses don't have those things. In particular:__\n",
    "\n",
    "- PoolQC --> meaning NO Swimming pool\n",
    "- Alley --> meaning NO Alley access\n",
    "- MiscFeature --> meaning NO Miscellaneous feature\n",
    "- Fence --> meaning No fence\n",
    "- ...\n",
    "\n",
    "\n",
    "__The once that instead have missing values because of wrong recording are:__\n",
    "\n",
    "- LotFrontage\t\n",
    "- GarageYrBlt\t\n",
    "- MasVnrArea\t\n",
    "\n",
    "__The picture below shows the 3 features with missing values (TRUE _'NaN'_):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_nan = ['LotFrontage', 'GarageYrBlt', 'MasVnrArea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the missing_df. missing_df is update with only the features with real missing values.\n",
    "missing_df = missing_df.loc[missing_df['feature_name'].isin(true_nan)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Histogram_NaN_Values(missing_df, real = 'yes', size = 30, x_n = 0.25)\n",
    "#plt.savefig('Plots/Histogram true NaN.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Because of this, for the first group of feautures we are going to subsitute the _'NaN'_ values with _'NO'_(meaning that these specif houses do not have this characteristic, and for the second group of features we substitute the _'NaN'_ with the _'mean value'_ based on their belonging \"neighborhood\".__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_nan_function(df, all_nan, true_nan):\n",
    "    \n",
    "    for f in all_nan:\n",
    "        if f not in true_nan: # if the feature is not into the 'true_nan' list, then replace the 'NaN' values with 'NO' \n",
    "            df[f].fillna('NO', inplace=True)\n",
    "            \n",
    "        else: # fill NaN values by using mean value computed based on the neighborhood\n",
    "            \n",
    "            df[f] = df.groupby(\"Neighborhood\")[f].transform(lambda x: x.fillna(x.mean()))\n",
    "            \n",
    "    \n",
    "    #df_train.fillna(df_train.median(), inplace=True) # replace all the remaining 'NaN' values with the 'median' value of the selected feature\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = fix_nan_function(df_train, all_nan, true_nan ) # call the function 'fix_nan_function'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del missing_df # delete df\n",
    "del all_nan # delete list\n",
    "del true_nan # delete list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum().sum() # check if we still have 'NaN' values. The output should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Delete outliers\n",
    "\n",
    "Create a new column with the outliers:\n",
    "\n",
    "- 'yes' if the sample has 'SalePrice' < 700000 and 'GrLivArea' > 4000.\n",
    "- 'no' if the two conditions are not satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['outlier'] = np.where((df_train['SalePrice']<700000) & (df_train['GrLivArea']>4000), 'yes', 'no') # add new column\n",
    "\n",
    "\n",
    "ax = sns.lmplot(x = 'GrLivArea', y = 'SalePrice', data = df_train,\n",
    "        hue=\"outlier\", fit_reg=False, markers=[\"o\", \"x\"], palette = [\"#2ecc71\", \"red\"],\n",
    "        aspect=2); # print liearmodel\n",
    "\n",
    "#plt.savefig('Plots/Outliers.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers\n",
    "df_train.drop(df_train[(df_train.SalePrice < 700000) & (df_train.GrLivArea > 4000)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x = 'GrLivArea', y = 'SalePrice', data = df_train,\n",
    "           hue=\"outlier\", fit_reg=False, markers=[\"o\"], palette = [\"#2ecc71\"],\n",
    "           aspect=2);\n",
    "#plt.savefig('Plots/Deleted_Outliers.png');\n",
    "\n",
    "df_train.drop(['outlier'], axis=1, inplace = True)# drop 'outlier' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Check distribtion of target features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a density plot and qq-plot for a selected feature\n",
    "def qq_plot(df, feature_name): \n",
    "    \n",
    "    df = df[feature_name]\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    \n",
    "    ax1 = fig.add_subplot(121) # distribution plot\n",
    "    ax2 = fig.add_subplot(122) # QQ Plot \n",
    "    \n",
    "    #distribution plot\n",
    "    sns.distplot(df,\n",
    "                 kde_kws={\"color\": \"g\", \"lw\": 3, \"label\": \"Kernal Density Estimation\"},\n",
    "                 hist_kws={\"histtype\": \"step\", \"linewidth\": 3,\n",
    "                           \"alpha\": 1, \"color\": \"b\"}, norm_hist = False, ax = ax1); # distribution plot\n",
    "\n",
    "    \n",
    "    stats.probplot(df, plot=sns.mpl.pyplot); # QQ Plot\n",
    "\n",
    "    \n",
    "    ax1.set_title(str(feature_name)+' Distribution', color='k', fontsize=25)\n",
    "    ax1.set(xlabel = feature_name)\n",
    "    ax1.xaxis.labelpad = 20\n",
    "    ax1.xaxis.label.set_size(20)\n",
    "    #ax2.set(xlabel='Sale Price')\n",
    "    ax2.set_title('QQ Plot ' + str(feature_name), color = 'k', fontsize=25)\n",
    "    ax2.xaxis.labelpad = 20\n",
    "    ax2.xaxis.label.set_size(20)\n",
    "    ax2.yaxis.label.set_size(20)\n",
    "    ax2.get_lines()[0].set_marker('+') # QQ plot markers\n",
    "    ax2.get_lines()[0].set_markersize(12.0) # QQ plot markers size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq_plot(df_train, feature_name = 'SalePrice')\n",
    "#plt.savefig('Plots/SalePrice qqplot no trasformation.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SalePrice Skewness: \"+str(round(df_train['SalePrice'].skew(),3)),\n",
    "      \"SalePrice Kurtosis: \"+str(round(df_train['SalePrice'].kurt(),3)),\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Targert transformation\n",
    "\n",
    "\n",
    "__As it is possible seeing, the target feature doesn't follow a normal distribution and this could reduce the performace of Machine Learning Algorithms. For that reason, different transformation are tried in order to find one that is able to reduce Skewness and Kurtosis.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'transformation_function' is a function that execute different transformation on a selected feature.\n",
    "\n",
    "def transformation_function(df, feature_name):\n",
    "    \n",
    "    T_results = defaultdict(list)\n",
    "    T_df_results = defaultdict()\n",
    "    \n",
    "    # ln(x)\n",
    "    SalePrice_Ln = np.log(df[feature_name]) # function computes the value of ln(x)\n",
    "    T_results[str(feature_name)+'_Ln'].extend([round(SalePrice_Ln.skew(),4),round(SalePrice_Ln.kurt(),4)]) #compute skewness and kurtosis\n",
    "    T_df_results[str(feature_name)+'_Ln'] = SalePrice_Ln\n",
    "       \n",
    "    #sqrt(x)\n",
    "    SalePrice_root= np.sqrt(df[str(feature_name)]) # returns the sqrt(x)\n",
    "    T_results[str(feature_name)+'_root'].extend([round(SalePrice_root.skew(),4),round(SalePrice_root.kurt(),4)]) #compute skewness and kurtosis\n",
    "    T_df_results[str(feature_name)+'_root'] = SalePrice_root\n",
    "    \n",
    "    #cbrt(x)\n",
    "    SalePrice_croot= np.cbrt(df[str(feature_name)]) # returns the cbrt(x)\n",
    "    T_results[str(feature_name)+'_croot'].extend([round(SalePrice_croot.skew(),4),round(SalePrice_croot.kurt(),4)]) #compute skewness and kurtosis\n",
    "    T_df_results[str(feature_name)+'_croot'] = SalePrice_croot\n",
    "    \n",
    "    #square(x)\n",
    "    SalePrice_square= np.square(df[str(feature_name)]) # returns the sqrt(x)\n",
    "    T_results[str(feature_name)+'_square'].extend([round(SalePrice_square.skew(),4),round(SalePrice_square.kurt(),4)]) #compute skewness and kurtosis\n",
    "    T_df_results[str(feature_name)+'_square'] = SalePrice_square\n",
    "    \n",
    "    return (T_results, T_df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that plot distribution of a particoular feature after 'transformation_function'\n",
    "def Skewness_Kurtosis_Plots(df, feature_name):\n",
    "    \n",
    "    T_results, T_df_results = transformation_function(df, feature_name)\n",
    "    \n",
    "    colors = ['darkorange', 'dodgerblue', 'seagreen', 'mediumvioletred', 'darkslategray', 'maroon']\n",
    "    key_list = [k for k in T_results.keys()] # list with all the SalePrice transformation\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "    \n",
    "    if feature_name == 'SalePrice':\n",
    "        fig.suptitle('Target Transformation Distribution', fontsize=25)\n",
    "    else:\n",
    "        fig.suptitle(feature_name+' Transformation Distribution', fontsize=25)\n",
    "\n",
    "    for n, k in enumerate(key_list):\n",
    "        try:# because if there are 0 it is not possible compute the log\n",
    "            ax = fig.add_subplot(2,2, (n+1))\n",
    "            sns.distplot(T_df_results[k], color=colors[n], kde_kws={\"label\": \"Kernal Density Estimation\"})\n",
    "            ax.set_title(k.split(\"_\")[1]+', Skewness: '+str(T_results[k][0])+' Kurtosis: '+str(T_results[k][1]), color='Red', fontsize=20)\n",
    "            ax.set(xlabel = k.split(\"_\")[1]+str(feature_name))\n",
    "            ax.xaxis.labelpad = 10\n",
    "            ax.xaxis.label.set_size(15)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Skewness_Kurtosis_Plots(df_train, feature_name='SalePrice')\n",
    "#plt.savefig('Plots/SalePrice trasformations.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Target Transformation\n",
    "\n",
    "__The picture above shows how the natural logarithm can imporve the Skewness and Kurtosis of the target feature.\n",
    "For that reason a new column called 'SalePrice_Ln' is added to df_train. This new column contains the natural logathm of the SalePrice values.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['SalePrice_Ln'] = np.log(df_train['SalePrice']) #add a new column with the SalePrice_Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq_plot(df_train, feature_name = 'SalePrice_Ln')\n",
    "#plt.savefig('Plots/SalePrice qqplot YES trasformation.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SalePrice Skewness: \"+str(round(df_train['SalePrice_Ln'].skew(),3)),\n",
    "      \"SalePrice Kurtosis: \"+str(round(df_train['SalePrice_Ln'].kurt(),3)),\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Numerical Features Selection and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Skewness_and_Kurtosis_num' is a function that compute Skewness and Kurtosis on the selected features. (In this case Numerical)\n",
    "def Skewness_and_Kurtosis_num (df_train, numerical_features):\n",
    "\n",
    "    numerical_S_and_K = defaultdict() # dictionary where--> 'Feature Name': [Skewness, Kurtosis]\n",
    "    for f in numerical_features:\n",
    "        numerical_S_and_K[f]= [round(df_train[f].skew(),3), round(df_train[f].kurt(),3)] # compute Skewness and Kurtosis\n",
    "\n",
    "    df = pd.DataFrame.from_dict(numerical_S_and_K, orient='index',columns = ['Skewness', 'Kurtosis']) # convert dictionary into df\n",
    "    return (df)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame of Skweness and and Kurtosis for numerical features\n",
    "df_numerical_S_and_K = Skewness_and_Kurtosis_num(df_train, numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__'pearson_numerical' function computes the pearson correlation coefficent for all the numerical variables.<br>\n",
    "The function returns the features with a correlation coefficent greater than 0.5 and a p-value less then 5%.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_numerical(df_train):\n",
    "    # features that we don't want to include in the pearson correlation computation\n",
    "    irrelevant_features = ['Id', 'SalePrice', 'SalePrice_Ln'] \n",
    "\n",
    "    #dictionary where --> 'Feature Name': [Pearson, p-value]\n",
    "    numerical_corr = {f: pearsonr(df_train['SalePrice'], df_train[f]) \n",
    "                      for f in numerical_features \n",
    "                      if f not in irrelevant_features}\n",
    "\n",
    "    # save all the features that have p-value < 5% and Pearson correaltion coefficent > 0.4\n",
    "    numerical_corr_selected = {k: [round(v[0],3),v[1]] for k,v in numerical_corr.items() if v[1] < 0.05 and v[0] > 0.5}\n",
    "    \n",
    "    R2 = [[k, round((v[0]**2)*100,3)] for k,v in numerical_corr_selected.items()] # compute the R^2\n",
    "    \n",
    "    return(numerical_corr_selected, R2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_numerical_corr, R2_numerical = pearson_numerical(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Pearson correlation coefficent for relevant Numerical Features\n",
    "\n",
    "fig = plt.figure(figsize=(20,50))\n",
    "colors = ['deepskyblue', 'seagreen', 'sandybrown', 'orchid', 'slateblue', 'olive', 'maroon', 'darkorange', 'slategrey', 'limegreen', 'deeppink', 'aquamarine', 'tomato','blue']\n",
    "for e, (k,v) in enumerate(relevant_numerical_corr.items()):\n",
    "    ax = fig.add_subplot(7, 2, (e+1))\n",
    "    sns.regplot(df_train[k], df_train['SalePrice'], marker = \"+\", color = colors[e]) # compute regression plot per each numerical selected feature\n",
    "    ax.set_title('Pearson: '+str(round(v[0],3))+', p-value: '+str(v[1]), color='r', fontsize=15)\n",
    "    ax.xaxis.label.set_size(12)\n",
    "    ax.yaxis.label.set_size(12)\n",
    "#plt.savefig('Plots/numerical correlation plot.png');  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_fun(lista): # function to sort 'R2_numerical'\n",
    "    lista.sort(key = lambda x: x[1], reverse=True) \n",
    "    return lista\n",
    "\n",
    "\n",
    "# Print R^2 for numerical features:\n",
    "fig = plt.figure(dpi=80)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "table = ax.table(cellText=sort_fun(R2_numerical), colLabels=['features','R^2'], loc='center')\n",
    "table[(0, 0)].set_facecolor(\"#56b5fd\")\n",
    "table[(0, 1)].set_facecolor(\"#1ac3f5\")\n",
    "table.set_fontsize(10)\n",
    "table.scale(1,2)\n",
    "ax.axis('off');\n",
    "\n",
    "#plt.savefig('Plots/R2 numerical features_0.5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 _'train'_ dataset definition \n",
    "\n",
    "__Create a new DataFrame called 'train' where we are going to store all the selected numerica features + the SalePrice_Ln__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print a corr matrix for most relevant 'numerical features'\n",
    "\n",
    "def Corr_matrix_plot(df, size_num, labelsize):\n",
    "    \n",
    "    corr = df.corr()\n",
    "    \n",
    "    # plot the figure\n",
    "    fig = plt.figure(figsize = (15,10))\n",
    "    plt.title('Numerical_Corr_Matrix', y=1.01, fontsize=20, color='r')\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.heatmap(corr,  cmap= 'YlGnBu', annot=True, annot_kws={\"size\": size_num}, linewidth=3, ax =ax);\n",
    "    ax.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "\n",
    "\n",
    "    ax.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train[[f for f in relevant_numerical_corr.keys()]]\n",
    "train['SalePrice_Ln'] = df_train['SalePrice_Ln'] # add also 'SalePrice_Ln'\n",
    "\n",
    "del relevant_numerical_corr\n",
    "del R2_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__It is possible seeing, from the figure above, how these features are high correlated with the _'Target Feature'_. Below are reported the 11 more relevant numerical features with their Pearson correlation coefficents and the p-values:__\n",
    "\n",
    "1. OverallQual ---> Pearson = 0.796, p-value ≈ 0. \n",
    "    \n",
    "- YearBuilt ---> Pearson = 0.524, p-value ≈ 0.\n",
    "    \n",
    "- YearRemodAdd ---> Pearson = 0.508, p-value ≈ 0.\n",
    "    \n",
    "- TotalBsmtSF ---> Pearson = 0.651, p-value ≈ 0.\n",
    "    \n",
    "- 1stFlrSF ---> Pearson = 0.632, p-value ≈ 0.\n",
    "    \n",
    "- GrLivArea ---> Pearson = 0.735, p-value ≈ 0.\n",
    "    \n",
    "- FullBath ---> Pearson = 0.562, p-value ≈ 0.\n",
    "    \n",
    "- TotRmsAbvGrd ---> Pearson = 0.538, p-value ≈ 0.\n",
    "    \n",
    "- GarageYrBlt ---> Pearson = 0.500, p-value ≈ 0.\n",
    "    \n",
    "- GarageCars ---> Pearson = 0.641, p-value ≈ 0.\n",
    "    \n",
    "- GarageArea ---> Pearson = 0.629, p-value ≈ 0.\n",
    "\n",
    "__Below a confusion matrix showing the correlation between the selected variables and the _'target feature'_:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_matrix_plot(train, size_num = 16, labelsize=12)\n",
    "#plt.savefig('Plots/numerical correlation matrix.png');  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Delete high correlated features\n",
    "\n",
    "\n",
    "- '1stFlrSF' is strongly correlated (0.8) with 'TotalBsmtSF'. For that reason we decide to delete '1stFlrSF' because it has a lower correlation with the 'SalePrice' feature.\n",
    "\n",
    "- 'GarageYrBlt' is strongly correlated (0.82) with 'YearBuilt'. For that reason we decide to delete 'GarageYrBlt' because it has a lower correlation with the 'SalePrice' feature.\n",
    "\n",
    "- 'TotRmsAbvGrd' is strongly correlated (0.83) with 'GrLivArea'. For that reason we decide to delete 'TotRmsAbvGrd' because it has a lower correlation with the 'SalePrice' feature.\n",
    "\n",
    "- 'GarageCars' is strongly correlated (0.89) with 'GarageArea'. For that reason we decide to delete 'GarageArea' because it has a lower correlation with the 'SalePrice' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['1stFlrSF', 'GarageYrBlt', 'TotRmsAbvGrd', 'GarageArea'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot qq-plot per each numerical variable\n",
    "\n",
    "for f in train.columns.tolist():\n",
    "    qq_plot(df_train, feature_name = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Skewness_Kurtosis per each numerical feature after 4 transformations\n",
    "\n",
    "for f in train.columns:\n",
    "    Skewness_Kurtosis_Plots(train, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 numerical features transformation \n",
    "\n",
    "__Now, it is necessary checking the distribution of all the selected numerical features. To do that, skewness and Kortosis are compute on all the 7 features __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_transformation(df, features):\n",
    "\n",
    "## COMPUTE AND DISPLAY TWO DATAFRAMES WHERE:\n",
    "# (1) s_k_before_transf => Dataframe with Skewness and Kurtosis of numerical features before transformations\n",
    "# (2) num_s_k => Dataframe with Skewness and Kurtosis of numerical features after transformations (log, sqrt, )\n",
    "\n",
    "\n",
    "    df_num_list = []\n",
    "    for k in features:\n",
    "\n",
    "        dizi, df12 = transformation_function(df, k) # call the 'transofrmation_function' \n",
    "        df_num_list.append(pd.DataFrame.from_dict(dizi, orient='index')) # append the dictionary per each feature with all the transofrmation and transform it into df\n",
    "\n",
    "\n",
    "    s_k_before_transf = Skewness_and_Kurtosis_num(df, features).reset_index() # call the 'Skewness_and_Kurtosis_num' function to return the skewness and kurtosis before the transformation\n",
    "    s_k_before_transf.columns = ['feature_name','Skewness','Kurtosis']\n",
    "    num_s_k = pd.concat(df_num_list)\n",
    "    del df_num_list\n",
    "    num_s_k = num_s_k.reset_index() # reset the index in order to have numbers instead of features names    \n",
    "    num_s_k.columns = ['feature_name','Skewness','Kurtosis'] # rename the columns \n",
    "\n",
    "    display(s_k_before_transf)\n",
    "    display(num_s_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Taking a look at the table displayed above, it is clear how these 4 features can benefit of a logarithm transformation: <br>\n",
    "<br>\n",
    "'GrLivArea', 'TotalBsmtSF', 'FullBath' and 'GarageCars'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function apply the transformation on selected the categorical features \n",
    "def numerical_transformation(df):\n",
    "    \n",
    "    # ln transformation on GrLivArea\n",
    "    df['GrLivArea_ln'] = np.log(df['GrLivArea'])\n",
    "    df.drop(['GrLivArea'], axis=1, inplace = True)\n",
    "\n",
    "\n",
    "    # apply ln transformation only on data that have TotalBsmtSF > 0.\n",
    "    df.loc[df['TotalBsmtSF'] > 0, 'TotalBsmtSF'] = np.log(df['TotalBsmtSF'])\n",
    "    df.rename(columns={'TotalBsmtSF':'TotalBsmtSF_ln'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    df.loc[df['FullBath'] > 0, 'FullBath'] = np.log(df['FullBath'])\n",
    "    df.rename(columns={'FullBath':'FullBath_ln'}, inplace=True)\n",
    "\n",
    "    df.loc[df['GarageCars'] > 0, 'GarageCars'] = np.log(df['GarageCars'])\n",
    "    df.rename(columns={'GarageCars':'GarageCars_ln'}, inplace=True)\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = numerical_transformation(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_matrix_plot(train, size_num = 12, labelsize=10.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Categorical Features Cleaning and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# box-plots per each categorical feature based on the 'SalePrice' target feature\n",
    "def Box_Plot_function(df, lista, rows, cols):\n",
    "        \n",
    "    fig = plt.figure(figsize=(25,90))\n",
    "    #colors = ['deepskyblue', 'seagreen', 'sandybrown', 'orchid', 'slateblue', 'olive', 'maroon', 'darkorange', 'slategrey', 'limegreen', 'deeppink', 'aquamarine', 'tomato']\n",
    "    for e, v in enumerate(lista):\n",
    "        ax = fig.add_subplot(rows, cols, (e+1))\n",
    "        sns.boxplot(x = 'SalePrice', y = v, orient = \"h\", data = df)\n",
    "        sns.swarmplot(x = 'SalePrice', y = v, data = df, color = \".15\")\n",
    "        #ax.set_title(str(v), color='r', fontsize=15)\n",
    "        #ax.xaxis.label.set_size(12)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.yaxis.label.set_size(20)\n",
    "        ax.set_xlabel('') \n",
    "        ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Box_Plot_function(df_train, categorical_features,15, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 ANOVA\n",
    "\n",
    "__A good approach to perform Analysis of variance is to group the continuous variable (SalePrice) using the classes present into every categorical variable and compute the variances of every class. The variances computed are compared with the overall variance. If the variances ,after grouping, fall down significantly, it means that the categorical variable can explain most of the variance of the continuous variable meaning that the observed variable is highly significant. If the variables have no correlation, then the variance in the groups is expected to be similar to the original variance.__\n",
    "\n",
    "- https://newonlinecourses.science.psu.edu/stat501/lesson/12/12.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE THE VARIANCE OF 'SalePrice' GROUPED BY CATEGORY PER EACH CATEGORICAL FEATURES AND RETURNS THE ONCE WHERE THERE IS \n",
    "# A DROP INTO THE VARIANCE.\n",
    "\n",
    "# I used the std instead of var because in this way I have smaller numbers that are easier to compare\n",
    "\n",
    "print(df_train['SalePrice'].std())\n",
    "for f in categorical_features:\n",
    "        \n",
    "    test1 = df_train[['SalePrice', f]]\n",
    "    print('\\n\\n',test1.groupby(f).std(), test1.groupby(f).count())\n",
    "    \n",
    "del test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Taking a look at the table returned above, it is shown how these 5 variables seems to be really significant in terms of variance. <br>\n",
    "For that reason, ANOVA comes up with these 5 categorical features:__\n",
    "1. MSZoning \n",
    "- Neighborhood\n",
    "- ExterQual\n",
    "- BsmtQual\n",
    "- KitchenQual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Label Encoding for Categorical Features:\n",
    "\n",
    "Once the categorical feauters with a stronge influence on the 'SalePrice' variable have been found, the next step is to encode the classes in each category in order to perform Machine Learning models.\n",
    "Performs the encoding is always an hot topic because there are several approaches that can be used. In this particular case it has been decided to assign to every class of every feature the most appropriate value. Taking a look at the box-plot can help to find these values. In fact, it is possible coming up with this encoding:\n",
    "\n",
    "1. <p>'MSZoning' &#8658; Define 4 classes</p>\n",
    "- <p>'Neighborhood' &#8658; Define 3 classes</p>\n",
    "- <p>'ExterQual'&#8658; Define 4 classes</p>\n",
    "- <p>'BsmtQual'&#8658; Define 5 classes</p>\n",
    "- <p>'KitchenQual'&#8658; Define 4 classes</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_list_seleceted = ['MSZoning', 'Neighborhood', 'ExterQual', 'BsmtQual', 'KitchenQual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add categorical features to our train\n",
    "train[categorical_list_seleceted] = df_train[categorical_list_seleceted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Box_Plot_function(df_train, categorical_list_seleceted, 5, 1)\n",
    "#plt.savefig('Plots/box plot categorical.png'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_encoding(train_cat, target):\n",
    "\n",
    "    # (1) ancoding for ['MSZoning', 'ExterQual', 'BsmtQual', 'KitchenQual']\n",
    "    encode_map = {'MSZoning': {'C (all)': 1, 'RM': 2, 'RH': 2, 'RL': 3, 'FV': 4, 'NO':1}, # 'NO' is also added becasue it is necessary when the 'test.csv' has to encoded.\n",
    "                  'ExterQual': {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1},\n",
    "                  'BsmtQual': {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'NO':1}, # 'NO' is also added becasue it is necessary when the 'test.csv' has to encoded.\n",
    "                  'KitchenQual': {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'N0':1} # 'NO' is also added becasue it is necessary when the 'test.csv' has to encoded.\n",
    "                 }\n",
    "\n",
    "    train_cat.replace(encode_map, inplace=True) # use repalce function to apply the encoding\n",
    "\n",
    "    # (2) For 'Neighborhood' Encoding we used another approach because there are 25 features.\n",
    "\n",
    "    Neighborhood_category_2 = ['Timber', 'ClearCr', 'NWAmes', 'Somerst', 'Crawfor', 'Veenker', 'CollgCr','Blmngtn', 'Gilbert'] # features with higher influence\n",
    "    Neighborhood_category_3 = ['StoneBr', 'NridgHt','NoRidge']#, 'SawyerW'] # features with medium influence\n",
    "\n",
    "    train_cat['Neighborhood_Encoded'] = 1  # set all == 1 \n",
    "    train_cat.loc[(train_cat['Neighborhood'].isin(Neighborhood_category_2)), 'Neighborhood_Encoded'] = 2  # change 1 with 2, if feature is in category_2 satisfied\n",
    "    train_cat.loc[(train_cat['Neighborhood'].isin(Neighborhood_category_3)), 'Neighborhood_Encoded'] = 3  # change 1 with 3, if feauter is in category_3 satisfied \n",
    "    \n",
    "    if target == 'yes':\n",
    "        train_cat = train_cat[[f for f in train_cat.columns.tolist() if f != 'SalePrice_Ln'] + ['SalePrice_Ln']] # change column order, because we want SalePrice_Ln at the end\n",
    "    else:\n",
    "        train_cat = train_cat[[f for f in train_cat.columns.tolist()]]\n",
    "    train_cat.drop(['Neighborhood'], axis=1, inplace = True) # drop the column with strings.\n",
    "    train_cat.rename(columns={'Neighborhood_Encoded': 'Neighborhood'}, inplace=True)\n",
    "    \n",
    "    \n",
    "    return(train_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = categorical_encoding(train, target= 'yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3 categorical features transformation\n",
    "\n",
    "__As done for numerical features, also categorical encoded features can be transformed if a significant improvement on Skewness and Kurtosis is reported.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform these four numerical features probabily we don't need.\n",
    "def categorical_transformation(df):\n",
    "    df['MSZoning_square'] =  np.square(df['MSZoning'])\n",
    "    df.drop(['MSZoning'], axis=1, inplace = True)\n",
    "\n",
    "    df['ExterQual_croot'] = np.cbrt(df['ExterQual'])\n",
    "    df.drop(['ExterQual'], axis=1, inplace = True)\n",
    "\n",
    "    df['KitchenQual_croot'] = np.log(df['KitchenQual'])\n",
    "    df.drop(['KitchenQual'], axis=1, inplace = True)\n",
    "\n",
    "    df['BsmtQual_root'] = np.sqrt(df['BsmtQual'])\n",
    "    df.drop(['BsmtQual'], axis=1, inplace = True)\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_transformation(train, categorical_list_seleceted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = categorical_transformation(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_matrix_plot(train, size_num = 12, labelsize=10)\n",
    "#plt.savefig('Plots/Correlation_matrix_categorical_and_numerical.png'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Standardize Data & Spliting in Train and Test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.copy()\n",
    "\n",
    "df.drop(['SalePrice_Ln'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit and transform the data (excluded target feature)\n",
    "df = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(data = df, columns = [f for f in train.columns if f not in ['SalePrice', 'SalePrice_Ln']])\n",
    "df = pd.DataFrame(data = df, columns = [f for f in train.columns if f not in ['SalePrice_Ln']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train and Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test set.\n",
    "X = df.copy()\n",
    "y = train['SalePrice_Ln']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3) # we split the df in train(70%) and test(30%) set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Base Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chosen base line is the median value.  \n",
    "baseline_prediction = y_test.median() # compute baseline\n",
    "absolute_error = abs((y_test - baseline_prediction).sum()) # compute absolute error\n",
    "print('MAE baseline: ', round(np.mean(absolute_error), 2)) # compute mean absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \n",
    "    'knn': KNeighborsRegressor(),\n",
    "    'dtr': DecisionTreeRegressor(),\n",
    "    'svm': SVR(),\n",
    "    'rf': RandomForestRegressor(),\n",
    "    'ada': AdaBoostRegressor()\n",
    "              }\n",
    "\n",
    "paramiters = {\n",
    "    \n",
    "    'knn':{'n_neighbors':[int(x) for x in np.linspace(1, 200, num = 5)],\n",
    "           'weights':['uniform','distance']},\n",
    "    \n",
    "    'dtr': {\n",
    "        'min_samples_split': [int(x) for x in np.linspace(2,40, num = 3)],\n",
    "        'max_depth': [int(x) for x in np.linspace(10, 100, 4)],\n",
    "        'min_samples_leaf': [int(x) for x in np.linspace(1, 5, 5)],\n",
    "        'max_features' : ['log2','sqrt', 5, None]\n",
    "    },\n",
    "    \n",
    "    'svm': [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4],\n",
    "                     'C': [10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [10, 100, 1000]}],\n",
    "    \n",
    "    'rf': {\n",
    "        'n_estimators': [int(x) for x in np.linspace(100,200, num = 2)],\n",
    "        'max_depth': [int(x) for x in np.linspace(10, 100, 2)],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'bootstrap': [True]\n",
    "    },\n",
    "    \n",
    "    'ada':{\n",
    "        'n_estimators': [500,1000,2000],\n",
    "        'learning_rate':[.001,0.01,.1]\n",
    "    }\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_scores = {} # dictionary where are stored all the scores \n",
    "GS_best_scores = {} # dictionary where are stored only the best scores per each model\n",
    "GS_best_paramiters = {} # dictionary where are stored only the best models\n",
    "GS_best_estimators = {}# dictionary where are are stored best estimators (so we don't need to write them)\n",
    "n = len(classifiers.keys())\n",
    "\n",
    "for k in classifiers.keys():\n",
    "    \n",
    "    # we run up to all the models are tuned\n",
    "    grid = GridSearchCV(\n",
    "        classifiers[k],\n",
    "        paramiters[k],\n",
    "        cv = KFold(n_splits=10, random_state=25, shuffle=True),scoring='r2')\n",
    "\n",
    "    grid.fit(X_train,y_train)\n",
    "    #fitted_models['rf'] = grid\n",
    "    GS_scores[k] = grid.cv_results_\n",
    "    GS_best_scores[k] = grid.best_score_\n",
    "    GS_best_paramiters[k] = grid.best_params_\n",
    "    GS_best_estimators[k] = grid.best_estimator_\n",
    "    n -=1\n",
    "    print('GS for {} completed.. still {} models to be processed!'.format(k,n),end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_best_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. R^2, MAE, RMSE for tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation = defaultdict()\n",
    "r2_score_models_dict = defaultdict()\n",
    "mae_models_dict = defaultdict()\n",
    "rmsr_dict = defaultdict()\n",
    "\n",
    "\n",
    "models = [k for k in GS_best_estimators.keys()] + ['mr']\n",
    "    \n",
    "for k in models: # now we call, one by one, all the models\n",
    "    \n",
    "    if k == 'mr':\n",
    "        model_selected = linear_model.LinearRegression()\n",
    "    else:\n",
    "    \n",
    "        model_selected = GS_best_estimators[k] # we select the model for the tuned models\n",
    "    cross_validation_score = cross_val_score(model_selected, X_train, y_train, cv=10)\n",
    "    model_selected.fit(X_train,y_train) # we fit the model with the train set\n",
    "    y_prediction = model_selected.predict(X_test) # we make the prediction\n",
    "    \n",
    "    \n",
    "    \n",
    "    cross_validation[k] = cross_validation_score.tolist() # store cross validation scores\n",
    "    r2_score_models_dict[k] = r2_score(y_test,y_prediction) # store r2\n",
    "    mae_models_dict[k] = mean_absolute_error(y_test, y_prediction) # store mean absolute erro\n",
    "    rmsr_dict[k] = np.sqrt(mean_squared_error(y_test, y_prediction)) # store root mean square error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Tuning Neural Network Paramiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2, 3, 4, 5] #array of layers\n",
    "neurones = [200, 225, 250, 275] #array of neurones\n",
    "epch = [10000, 15000, 20000, 25000] #array of epoch\n",
    "\n",
    "NN_accuracy = defaultdict(list) #dictionary where we store results\n",
    "\n",
    "#Tuning the parameters\n",
    "\n",
    "#to loop the layers array\n",
    "for x in layers:\n",
    "            \n",
    "    #create the model when number of layers equals to 2\n",
    "    if(x == 2):\n",
    "        #create the model with 200 neurones, 10000 epoches and 2 layers\n",
    "        reg = MLPRegressor(activation = 'relu',hidden_layer_sizes = (neurones[0],neurones[0]),\n",
    "                           max_iter=epch[0],learning_rate='adaptive',\n",
    "                           tol=0.0,warm_start=True,solver='adam')\n",
    "        \n",
    "    #create the model when number of layers equals to 3       \n",
    "    if(x == 3):\n",
    "        #create the model with 200 neurones, 10000 epoches and 3 layers\n",
    "        reg = MLPRegressor(activation = 'relu',hidden_layer_sizes = (neurones[0],neurones[0], neurones[0]),\n",
    "                           max_iter=epch[0],learning_rate='adaptive',\n",
    "                           tol=0.0,warm_start=True,solver='adam')\n",
    "      \n",
    "    #create the model when number of layers equals to 4 \n",
    "    if(x == 4):\n",
    "        #create the model with 200 neurones, 10000 epoches and 4 layers\n",
    "        reg = MLPRegressor(activation = 'relu',hidden_layer_sizes = (neurones[0],neurones[0],neurones[0],neurones[0]),\n",
    "                            max_iter=epch[0],learning_rate='adaptive',\n",
    "                            tol=0.0,warm_start=True,solver='adam')\n",
    "            \n",
    "    #create the model when number of layers equals to 5\n",
    "    if(x == 5):\n",
    "        #create the model with 200 neurones, 10000 epoches and 5 layers\n",
    "        reg = MLPRegressor(activation = 'relu',hidden_layer_sizes = (neurones[0],neurones[0],neurones[0],neurones[0],neurones[0]),\n",
    "                            max_iter=epch[0],learning_rate='adaptive',\n",
    "                            tol=0.0,warm_start=True,solver='adam')\n",
    "\n",
    "    #applying 10 fold cross validation\n",
    "    tst = cross_val_score(reg, X_train, y_train, cv = 10)\n",
    "\n",
    "    #save results to the NN_accuracy\n",
    "    NN_accuracy[\"acc_layer\"].append(tst)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#check the accuracy by changing number of neurones\n",
    "for x in neurones:\n",
    "    \n",
    "    #create the model 10000 epoches and 2 layers with number of neurones change with each iteration according to the 'neurones' array\n",
    "    reg = MLPRegressor(activation = 'relu',hidden_layer_sizes = (x,x),\n",
    "                       max_iter=epch[0],learning_rate='adaptive',\n",
    "                       tol=0.0,warm_start=True,solver='adam')\n",
    "    \n",
    "    #applying 10 fold cross validation\n",
    "    tst = cross_val_score(reg, X_train, y_train, cv = 10)\n",
    "\n",
    "    #save results to the NN_accuracy\n",
    "    NN_accuracy[\"acc_neurones\"].append(tst)\n",
    "\n",
    "\n",
    "#check the accuracy by changing number of iterations\n",
    "for x in epch:\n",
    "        \n",
    "    #create the model 200 neurones and 2 layers with number of epoches change with each iteration according to the 'epch' array\n",
    "    reg = MLPRegressor(activation = 'relu',hidden_layer_sizes = (100,100),\n",
    "                       max_iter=x,learning_rate='adaptive',\n",
    "                       tol=0.0,warm_start=True,solver='adam')\n",
    "        \n",
    "    #applying 10 fold cross validation\n",
    "    tst = cross_val_score(reg, X_train, y_train, cv = 10)\n",
    "    \n",
    "    #save results to the NN_accuracy\n",
    "    NN_accuracy[\"acc_epoch\"].append(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array to store the mean results of each cross validation\n",
    "nn_parameters = defaultdict(list)\n",
    "\n",
    "#loop keys of the NN_accuracy\n",
    "for k in NN_accuracy.keys():\n",
    "    \n",
    "    for v in NN_accuracy[k]:\n",
    "        #calculate the mean for each section of tuning parameters\n",
    "        nn_parameters[k].append(np.mean(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create the model for the best chosen parameters(no-of-layers - 2, neurones - 200, epoches - 10000)\n",
    "reg = MLPRegressor(activation = 'relu',hidden_layer_sizes = (200,200),\n",
    "                       max_iter=10000,learning_rate='adaptive',\n",
    "                       tol=0.0,warm_start=True,solver='adam')\n",
    "        \n",
    "#applying 10 fold cross validation\n",
    "cross_validation['nn'] = cross_val_score(reg, X_train, y_train, cv = 10).tolist() # save cross validation results for the selected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 R^2, mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "r2_score_models_dict['nn'] = r2_score(y_test, y_pred) # store r^2 into 'r2_score_models_dict'\n",
    "mae_models_dict['nn'] = mean_absolute_error(y_test, y_pred) # store mae into 'mae_models_dict'\n",
    "rmsr_dict['nn'] = np.sqrt(mean_squared_error(y_test, y_pred)) # store rmse into 'rmsr_dict'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plot Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert R^2, mean_absolute_error and root_mean_square_error into DataFrame because we need for histograms__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame where we there are all the results:\n",
    "df_results = pd.DataFrame.from_records([r2_score_models_dict, mae_models_dict, rmsr_dict] +\n",
    "                                       [{k: np.square(v) for k,v in rmsr_dict.items()}],\n",
    "                                       index=['r2', 'mae','rmse', 'mse']).T.sort_values(by= 'rmse', ascending=False).reset_index().rename(columns={'index':'models'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Box-Plot models comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in cross_validation.items():\n",
    "    print(k, np.mean(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, data = [*zip(*cross_validation.items())] \n",
    "labels, data = cross_validation.keys(), cross_validation.values()\n",
    "\n",
    "plt.figure(figsize = (15,12))\n",
    "plt.subplots_adjust(left=0.25)\n",
    "plt.boxplot(data)\n",
    "\n",
    "plt.title('Model Comparison', fontsize=18, color = 'green')\n",
    "plt.xticks(range(1, len(labels) + 1), labels, fontsize = 15)\n",
    "plt.xlabel('models', fontsize=18).set_color(\"Blue\")\n",
    "plt.ylabel('R^2', fontsize=18).set_color(\"Blue\")\n",
    "\n",
    "plt.savefig('modelsR2_strange2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Regression matrics plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,15))\n",
    "\n",
    "for n,k in enumerate([l for l in df_results.columns.tolist() if l != 'models']):\n",
    "    \n",
    "    ax = fig.add_subplot(2, 2, (n+1))\n",
    "    sns.barplot(x = 'models', y = k,data = df_results)\n",
    "    ax.set_title(k, color='Red', fontsize=25)\n",
    "    #ax.xaxis.label.set_size(25)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    #plt.setp(ax,yticks=[0,5])\n",
    "    #ax.tick_params(axis='y', which='minor', labelsize=15)\n",
    "    #sns.set(font_scale = [1 if n in [0,6] else 1.5][0])\n",
    "    sns.set(font_scale = 1.5)\n",
    "   \n",
    "  \n",
    "    \n",
    "    for e,p in enumerate(ax.patches):\n",
    "        values = str(round(df_results[k][e], 4))\n",
    "        x = p.get_x() + 0.01 # get x coordinate\n",
    "        y = p.get_y() + p.get_height() + 0.005 # get y coordinate\n",
    "        ax.annotate(values, (x, y),fontsize=15)\n",
    "    \n",
    "plt.savefig('regressionmatric2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Cleaning and Kaggle Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Import and prepare the test.csv by calling the predefined functions:\n",
    "1. 'fix_nan_function'\n",
    "- 'categorical_encoding'\n",
    "- 'categorical_transformation'\n",
    "- 'numerical_transformation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "\n",
    "test = df_test[['OverallQual', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF',\n",
    "       'FullBath', 'GarageCars', 'GrLivArea', 'Neighborhood', 'MSZoning', 'ExterQual',\n",
    "       'KitchenQual', 'BsmtQual']] # select all the most relevant features\n",
    "\n",
    "test = fix_nan_function(test,\n",
    "                        ['BsmtQual', 'MSZoning', 'KitchenQual', 'GarageCars', 'TotalBsmtSF'],\n",
    "                        ['GarageCars', 'TotalBsmtSF']) # fix the 'NaN' values by calling the predefined 'fix_nan_function'\n",
    "\n",
    "# apply encoding using predefined function\n",
    "test = categorical_encoding(test, target='no') \n",
    "mapping = {'NO': 1}\n",
    "test.replace(mapping, inplace=True)\n",
    "\n",
    "test = categorical_transformation(test) # categorical transforamtion\n",
    "test = numerical_transformation(test) # numerical transforamtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize test data\n",
    "scaler = StandardScaler()\n",
    "test_transformed = scaler.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Prediction by using the best model, Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = GS_best_estimators['rf'].predict(test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Store predictions into .csv file and upload on Kaggle platform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame()\n",
    "df_predictions['Id'] = df_test['Id']\n",
    "df_predictions['SalePrice'] = np.exp(predictions)\n",
    "\n",
    "#pred_df.head(100)\n",
    "#df_predictions.to_csv('submission_rf.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
